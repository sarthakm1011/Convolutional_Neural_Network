{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print 20 // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The compose function allows for multiple transforms\n",
    "#transforms.ToTensor() converts our PILImage to a tensor of shape (C x H x W) in the range [0,1]\n",
    "#transforms.Normalize(mean,std) normalizes a tensor to a (mean, std) for (R, G, B)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download=False, transform=transform)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#Training\n",
    "n_training_samples = 20000\n",
    "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "#Validation\n",
    "n_val_samples = 5000\n",
    "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 5000\n",
    "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "class SimpleCNN(torch.nn.Module):\n",
    "    \n",
    "    #Our batch shape for input x is (3, 32, 32)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        #Input channels = 3, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = torch.nn.Linear(18 * 16 * 16, 64) #4608 input features, 64 output features  \n",
    "        self.fc2 = torch.nn.Linear(64, 10) #64 input features, 10 output features \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x)) #Size changes from (3, 32, 32) to (18, 32, 32)\n",
    "        x = self.pool(x) #Size changes from (18, 32, 32) to (18, 16, 16)\n",
    "        x = x.view(-1,18*16*16)  #Size changes from (18, 16, 16) to (1, 4608)\n",
    "        x = F.relu(self.fc1(x)) #Size changes from (1, 4608) to (1, 64)\n",
    "        x = self.fc2(x) #Size changes from (1, 64) to (1, 10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def outputSize(in_size,kernel_size, stride, padding):\n",
    "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "    return output\n",
    "    \n",
    "def get_train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_sampler,num_workers=2)\n",
    "    return train_loader\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1024, sampler=test_sampler, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.01):  \n",
    "    loss = torch.nn.CrossEntropyLoss() #Loss function\n",
    "    optimizer = optim.Adam(net.parameters(),lr=learning_rate) #Optimizer\n",
    "    return loss, optimizer\n",
    "    \n",
    "    \n",
    "def training(net, batch_size, n_epochs, learning_rate):\n",
    "        # Print all the hyperparameters\n",
    "        print '-'*10 + 'HYPER-PARAMETERS' + '-'*10\n",
    "        print 'Batch_size=' + str(batch_size) + ', Epochs=' + str(n_epochs) + ', Learning_Rate=' + str(learning_rate)\n",
    "        \n",
    "        #Get Trainig Data\n",
    "        train_loader = get_train_loader(batch_size)\n",
    "        n_batches = len(train_loader)\n",
    "        \n",
    "        #Creating loss and optimizer\n",
    "        loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "        \n",
    "        # Start Time\n",
    "        training_start_time = time.time()\n",
    "        print 'Trainning Start Time:' + str(training_start_time)\n",
    "        \n",
    "        # Loop for n_epochs\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            print_every = n_batches / 10\n",
    "            start_time = time.time()\n",
    "            total_train_loss = 0.0\n",
    "            \n",
    "            for i, data in enumerate(train_loader,0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                \n",
    "                #Set the parameter gradients to 0,To clean gradients that might be left in buffer \n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                #print inputs.size() \n",
    "                # Forward, Backward, Optimize\n",
    "                outputs = net(inputs)\n",
    "                loss_size = loss(outputs, labels)\n",
    "                loss_size.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Print Statistics\n",
    "                running_loss += loss_size.data[0]\n",
    "                total_train_loss += loss_size.data[0]\n",
    "                \n",
    "                # Print every 10th batch of an epoch\n",
    "                if (i+1) % (print_every+1) == 0:\n",
    "                    print 'Epoch: ' + str(epoch+1)\n",
    "                    print ' What is this? ' + str(100*(i+1)/n_batches)\n",
    "                    print 'Train Loss ' + str(running_loss)\n",
    "                    print 'Time Taken ' + str(time.time() - start_time)\n",
    "                    running_loss = 0.0\n",
    "                    start_time = time.time()\n",
    "                \n",
    "        # Do a pass on validation set \n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            #wrap tensor in variables\n",
    "            inputs, labels  = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward Pass \n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data[0]\n",
    "        \n",
    "        print 'Validation loss: ' + str(total_val_loss / len(val_loader))\n",
    "        print 'Training Finished, Time Taken: ' + str(time.time() - training_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------HYPER-PARAMETERS----------\n",
      "Batch_size=32, Epochs=1, Learning_Rate=0.001\n",
      "Trainning Start Time:1531124246.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:58: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:59: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      " What is this? 10\n",
      "Train Loss tensor(65.3550)\n",
      "Time Taken 3.6514518261\n",
      "Epoch: 1\n",
      " What is this? 20\n",
      "Train Loss tensor(63.0640)\n",
      "Time Taken 3.59728717804\n",
      "Epoch: 1\n",
      " What is this? 30\n",
      "Train Loss tensor(64.6717)\n",
      "Time Taken 3.91452789307\n",
      "Epoch: 1\n",
      " What is this? 40\n",
      "Train Loss tensor(68.2900)\n",
      "Time Taken 3.41613888741\n",
      "Epoch: 1\n",
      " What is this? 50\n",
      "Train Loss tensor(65.6449)\n",
      "Time Taken 3.2998418808\n",
      "Epoch: 1\n",
      " What is this? 60\n",
      "Train Loss tensor(65.2853)\n",
      "Time Taken 3.2302479744\n",
      "Epoch: 1\n",
      " What is this? 70\n",
      "Train Loss tensor(66.4568)\n",
      "Time Taken 3.24578881264\n",
      "Epoch: 1\n",
      " What is this? 80\n",
      "Train Loss tensor(63.7955)\n",
      "Time Taken 3.77501988411\n",
      "Epoch: 1\n",
      " What is this? 90\n",
      "Train Loss tensor(64.1204)\n",
      "Time Taken 3.64997696877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:80: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: tensor(1.1648)\n",
      "Training Finished, Time Taken: 38.7686150074\n"
     ]
    }
   ],
   "source": [
    "CNN = SimpleCNN()\n",
    "training(CNN, batch_size=32, n_epochs=1, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 9 5 1 8 4 7 0 0 2 9 7 6 4 5 9 1 7 4 0 8 9 1 6 5 6 3 1 3 6 5 4 0 9 9 7\n",
      " 4 3 8 1 4 6 0 4 0 6 2 4 3 3 7 0 9 1 2 3 9 0 0 7 0 0 8 4 5 1 6 8 9 9 3 0 5\n",
      " 2 7 6 1 2 0 3 3 7 6 3 7 9 4 4 9 8 9 6 2 3 6 3 9 2 0 4 4 3 5 2 0 4 4 5 9 2\n",
      " 7 6 3 0 8 9 6 0 3 3 3 1 7 0 4 8 1 3 4 3 6 4 3 7 2 4 8 0 0 1 3 3 0 9 6 9 4\n",
      " 9 4 5 3 2 3 3 8 2 7 0 3 4 6 1 2 0 3 1 3 7 6 6 6 8 1 9 3 1 9 3 8 3 3 0 4 4\n",
      " 7 0 2 1 8 7 5 3 9 8 6 6 5 1 2 2 1 3 4 4 4 2 3 8 1 4 0 8 8 0 4 9 2 5 7 2 7\n",
      " 4 1 3 3 0 4 8 8 8 8 5 8 6 1 6 4 1 9 3 3 7 9 5 1 4 1 4 1 9 8 9 1 9 9 1 1 1\n",
      " 9 2 6 0 5 9 1 4 0 1 1 7 9 9 4 9 1 7 4 6 9 5 1 7 0 6 2 4 8 8 9 9 8 0 0 1 8\n",
      " 1 8 2 0 0 1 3 4 0 8 2 0 9 0 9 3 6 6 0 6 3 2 1 6 7 1 3 4 3 1 9 3 2 3 3 6 1\n",
      " 8 8 1 1 5 6 0 8 6 1 9 0 2 8 9 8 0 9 6 8 3 1 4 8 3 9 5 3 2 1 7 1 0 9 9 3 5\n",
      " 9 4 0 6 6 0 6 5 4 8 3 2 4 7 1 5 3 5 6 0 0 7 2 1 7 1 4 6 3 4 0 8 1 5 9 2 5\n",
      " 2 0 4 4 4 2 8 1 1 1 1 1 1 9 0 4 1 3 1 8 6 4 5 3 1 2 6 5 4 4 6 8 3 6 3 3 3\n",
      " 0 0 7 0 8 3 0 8 3 1 1 1 1 1 3 9 5 2 1 9 9 3 4 3 6 6 7 9 0 7 5 4 0 8 1 4 2\n",
      " 1 3 6 7 7 0 4 0 8 6 6 9 0 6 1 3 0 3 5 3 6 1 4 4 0 8 5 6 9 1 5 3 0 3 6 6 6\n",
      " 2 7 3 3 4 6 7 8 8 3 8 1 0 3 4 3 3 0 3 4 5 1 4 1 3 4 2 4 3 8 3 3 8 6 5 7 7\n",
      " 5 7 0 3 0 8 3 6 4 4 7 4 4 6 1 7 2 9 5 3 2 1 4 6 4 6 6 1 3 2 4 7 0 1 7 6 0\n",
      " 3 9 6 7 8 9 3 9 9 4 3 3 7 9 7 8 3 9 0 4 8 1 6 6 7 6 7 2 7 5 7 5 9 6 6 3 9\n",
      " 6 2 4 6 6 7 4 6 3 4 0 7 3 0 0 6 1 7 5 9 1 5 2 4 3 8 3 1 9 3 1 5 9 1 8 3 0\n",
      " 0 3 0 3 7 1 4 0 8 7 6 0 0 3 7 6 1 2 7 3 2 3 7 3 9 2 5 5 8 4 0 6 3 3 3 4 6\n",
      " 8 4 7 4 6 0 4 6 0 1 1 1 2 7 4 1 0 8 6 1 4 8 9 5 8 3 1 1 4 1 9 2 6 7 3 3 2\n",
      " 5 4 6 7 1 6 4 5 7 8 6 3 1 5 2 9 3 3 1 3 1 6 1 8 7 4 7 5 3 3 0 3 8 1 7 7 7\n",
      " 4 9 0 0 9 8 4 4 3 9 7 3 5 4 7 4 8 6 3 1 7 5 5 2 6 9 8 6 9 1 7 0 0 9 0 0 6\n",
      " 8 8 3 2 9 1 2 1 3 2 3 0 5 7 6 7 4 4 0 2 4 3 6 6 3 8 5 6 8 5 7 2 3 5 8 6 6\n",
      " 8 3 7 4 1 1 7 4 1 0 7 4 7 7 9 3 2 8 9 1 4 1 2 1 8 8 3 6 0 9 2 4 7 5 6 1 3\n",
      " 9 8 9 6 2 0 3 8 6 4 3 4 8 9 8 2]\n",
      "[2 7 9 2 1 8 4 7 0 0 5 9 4 4 2 5 9 4 7 2 8 8 9 1 0 4 2 5 1 3 6 5 4 0 3 9 4\n",
      " 4 9 2 9 4 6 2 6 0 9 2 4 4 2 2 8 9 1 3 8 9 0 0 4 0 0 0 3 5 8 2 8 1 9 9 2 7\n",
      " 3 7 6 1 0 0 3 9 7 6 5 7 9 3 5 9 8 9 7 4 3 6 7 8 2 2 6 4 3 5 0 3 5 4 5 9 7\n",
      " 7 4 7 0 8 9 6 0 3 3 3 8 7 3 4 8 1 5 6 8 6 4 5 7 5 5 1 0 0 3 3 0 0 9 4 9 4\n",
      " 9 5 5 5 2 3 7 8 4 7 0 2 4 3 1 5 0 8 1 4 7 5 0 6 0 1 9 5 1 9 4 0 3 5 4 4 7\n",
      " 7 0 2 1 8 7 5 2 9 8 6 6 5 1 9 2 1 4 4 4 2 4 2 8 1 2 5 8 8 7 2 9 2 5 7 2 4\n",
      " 4 1 6 4 0 4 8 8 8 8 2 8 6 1 6 4 1 9 5 2 7 9 5 3 4 1 5 1 9 9 9 1 9 9 0 1 8\n",
      " 1 2 6 0 5 3 1 7 0 1 1 7 1 9 4 1 9 7 2 6 0 3 1 7 0 6 4 4 8 8 1 9 8 0 8 2 8\n",
      " 1 8 2 9 0 1 3 6 8 8 2 0 1 0 9 5 6 6 0 8 3 6 9 6 7 1 6 2 3 1 0 3 0 3 5 6 1\n",
      " 0 8 9 9 5 4 0 8 6 1 9 4 2 8 1 8 8 9 6 8 4 1 4 8 3 1 2 5 7 1 5 1 0 9 8 7 5\n",
      " 0 2 8 6 6 2 6 3 4 8 2 2 4 7 1 5 5 7 2 0 1 7 2 1 7 1 7 6 3 7 0 8 1 5 9 6 5\n",
      " 2 1 4 4 3 6 8 1 1 8 1 1 1 1 8 2 9 4 9 8 6 4 7 3 1 2 2 2 4 3 6 8 4 5 3 3 7\n",
      " 2 7 7 8 8 2 0 8 6 1 1 1 1 1 3 9 5 4 9 3 3 3 4 3 6 5 5 3 8 7 5 7 8 8 1 7 2\n",
      " 1 4 2 5 7 0 4 1 0 6 6 8 0 8 1 4 0 3 5 3 6 1 5 5 8 8 5 0 3 1 5 4 0 6 6 6 6\n",
      " 2 0 3 3 7 6 5 8 8 2 1 1 2 3 4 6 5 0 5 4 5 1 4 0 2 6 2 7 6 9 7 5 8 6 5 7 2\n",
      " 3 7 0 2 0 8 5 6 3 4 0 5 9 6 1 7 2 9 5 7 4 1 4 6 4 6 6 1 3 2 2 7 2 9 5 4 1\n",
      " 2 1 6 2 9 9 3 1 9 1 7 4 7 9 7 8 3 1 8 4 8 1 2 6 7 6 4 2 4 5 8 3 9 3 6 5 1\n",
      " 6 6 7 6 6 7 4 6 5 4 0 4 4 2 4 6 1 7 5 8 1 7 6 4 6 5 2 1 9 3 1 3 1 1 1 0 7\n",
      " 0 0 0 3 7 1 4 8 8 7 6 0 4 2 7 6 1 7 4 5 3 3 4 3 9 2 5 5 7 5 0 6 3 4 3 2 6\n",
      " 8 6 7 2 6 0 3 6 0 1 0 1 2 7 5 1 0 0 6 0 4 5 9 5 8 3 1 9 4 9 9 2 6 7 4 5 0\n",
      " 2 4 3 7 8 2 4 3 2 8 6 7 9 5 0 9 2 5 0 3 1 6 1 8 7 4 7 4 4 3 0 3 8 1 7 7 7\n",
      " 2 0 0 0 9 1 2 2 5 9 5 4 5 4 7 3 8 6 3 1 9 5 2 2 6 9 8 6 8 1 5 9 5 9 8 0 3\n",
      " 1 8 3 3 8 1 2 1 6 9 3 0 3 7 4 7 4 3 0 4 4 3 6 6 6 8 3 6 8 5 7 6 8 7 8 2 6\n",
      " 8 7 7 4 7 1 7 4 1 8 2 2 7 7 9 3 2 8 4 1 7 1 2 6 8 1 5 6 9 9 4 4 7 3 6 9 3\n",
      " 3 8 4 6 6 7 2 8 6 7 7 4 8 0 9 4]\n",
      "58.8495575221%\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "    test_out = CNN(torch.FloatTensor(inputs))\n",
    "    outs = test_out.data.numpy().argmax(axis=1)\n",
    "    y_test = labels.data.numpy()\n",
    "    #accuracy = (test_outputs == labels).sum()*100 / labels.shape\n",
    "\n",
    "print outs\n",
    "print y_test\n",
    "accuracy = (outs == y_test).sum()*100.0 / y_test.shape[0] \n",
    "print str(accuracy) + '%'\n",
    "\n",
    "# WHY are the number of samples equal to batch size in this????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1.weight', 'conv1.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias']\n",
      "torch.Size([18, 3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AFTER CNN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print CNN.state_dict().keys()\n",
    "\n",
    "conv_01 = CNN.state_dict()['conv1.weight']\n",
    "print conv_01.size()\n",
    "\n",
    "plt.figure(0)\n",
    "for ix in range(conv_01.shape[0]):\n",
    "    plt.subplot(5, 5, ix+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(conv_01[ix].reshape((3,9)), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# What are these exactly??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "# These are just activations of clusters percieved by eyes\n",
    "# Kerchevky's ImageNet paper\n",
    "# CNN have local connections, layering and spatial invariance\n",
    "# invariance is achieved by pooling \n",
    "\n",
    "# input --> feature learning --> classification\n",
    "# feature learning => convolution + relu --> pooling\n",
    "# classification => flatten --> fully connected --> softmax\n",
    "\n",
    "# dropout paper hinton (ovrefitting)\n",
    "# randomly picks neurons and sets the value to 0 so that the nn is learn from new paths\n",
    "# Andrey karpati cnn\n",
    "\n",
    "# use cnn when there is spatial variance for example if 2 columns are \n",
    "# swapped in position then it chnages data. use nn only at that time\n",
    "\n",
    "# Can build a custom filter also\n",
    "# Filter 1 = Sharpen\n",
    "#  0 -1   0\n",
    "# -1  5  -1\n",
    "#  0 -1   0\n",
    "\n",
    "# Filter 2 = Blur\n",
    "# 1 1 1 \n",
    "# 1 1 1 \n",
    "# 1 1 1 \n",
    "\n",
    "\n",
    "# Filter 3 = Edge Enhance\n",
    "#  0 0 0 \n",
    "# -1 1 0 \n",
    "#  0 0 0\n",
    "\n",
    "# Filter 4 = Edge Detect\n",
    "#  0  1  0\n",
    "#  1 -4  1\n",
    "# -0  1  0\n",
    "\n",
    "# It's not fully connected network \n",
    "# At each layer it goes to higher level of detection \n",
    "# eg. raw-pixels --> edges --> shapes --> facial shapes\n",
    "\n",
    "# Plotly install and implement but it's computationally exhaustive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
